## Simple 2D forward and gradient pass

!!! note "Add needed dependencies"
    Make sure your environment has the needed packages by doing:
    ```julia
    julia> ]
    (v1.9) activate /path/to/your/environment 
    (venv) add MPI Zygote CUDA ParametricDFNOs
    ```

To ensure that you have things set up properly and benchmark your results, you could run the following code:

```julia
using ParametricDFNOs.DFNO_2D
using MPI
using Zygote
using CUDA

MPI.Init()

comm = MPI.COMM_WORLD
rank = MPI.Comm_rank(comm)
size = MPI.Comm_size(comm)

partition = [1,size]

@assert MPI.Comm_size(comm) == prod(partition)

modelConfig = DFNO_2D.ModelConfig(nx=20, ny=20, nt=30, mx=4, my=4, mt=4, nblocks=3, partition=partition)

model = DFNO_2D.Model(modelConfig)
θ = DFNO_2D.initModel(model)

input_size = (model.config.nc_in * model.config.nx * model.config.ny * model.config.nt)
output_size = input_size * model.config.nc_out ÷ model.config.nc_in

# Move input and output to GPU, ignore x since it gets moved in the forward pass
x_sample = rand(modelConfig.dtype, input_size, 1)
y_sample = cu(rand(modelConfig.dtype, output_size, 1))

@time y = DFNO_2D.forward(model, θ, x_sample)
@time y = DFNO_2D.forward(model, θ, x_sample)
@time y = DFNO_2D.forward(model, θ, x_sample)

function loss_helper(params)
    global loss = dist_loss(DFNO_2D.forward(model, params, x_sample), y_sample)
    return loss
end

rank == 0 && println("STARTED GRADIENT SCALING")

@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]
@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]
@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]

MPI.Finalize()
```

Run the above by doing:

```shell

```

## Training 2D Time varying FNO

```julia

```
