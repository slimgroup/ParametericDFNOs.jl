var documenterSearchIndex = {"docs":
[{"location":"examples/simple_3D/#Simple-3D-forward-and-gradient-pass","page":"3D Forward and Gradient","title":"Simple 3D forward and gradient pass","text":"","category":"section"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"note: Jump right in\nTo get started, you can run some examples","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"Make sure to add necessary dependencies. You might also need to load a proper MPI implementation based on your hardware.","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"julia> ]\n(v1.9) activate /path/to/your/environment \n(venv) add MPI Zygote CUDA ParametricDFNOs","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"warning: To run on multiple GPUs\nIf you wish to run on multiple GPUs, make sure the GPUs are binded to different tasks. The approach we use is to unbind our GPUs on request and assign manually:CUDA.device!(rank % 4)which might be different if you have more or less than 4 GPUs per node. Also, make sure your MPI distribution is functional.","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"using MPI\nusing CUDA\nusing Zygote\nusing ParametricDFNOs.DFNO_3D\nusing ParametricDFNOs.UTILS\n\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\npe_count = MPI.Comm_size(comm)\n\nglobal gpu_flag = parse(Bool, get(ENV, \"DFNO_3D_GPU\", \"0\"))\nDFNO_3D.set_gpu_flag(gpu_flag)\n\n# Julia requires you to manually assign the gpus, modify to your case.\nDFNO_3D.gpu_flag && (CUDA.device!(rank % 4))\npartition = [1, pe_count]\n\nnx, ny, nz, nt = 20, 20, 20, 30\nmodes, nblocks = 8, 4\n\n@assert MPI.Comm_size(comm) == prod(partition)\nmodelConfig = DFNO_3D.ModelConfig(nx=nx, ny=ny, nz=nz, nt=nt, mx=modes, my=modes, mz=modes, mt=modes, nblocks=nblocks, partition=partition, dtype=Float32)\n\nmodel = DFNO_3D.Model(modelConfig)\nθ = DFNO_3D.initModel(model)\n\ninput_size = (model.config.nc_in * model.config.nx * model.config.ny * model.config.nz * model.config.nt) ÷ prod(partition)\noutput_size = input_size * model.config.nc_out ÷ model.config.nc_in\n\nx_sample = rand(modelConfig.dtype, input_size, 1)\ny_sample = rand(modelConfig.dtype, output_size, 1)\n\nDFNO_3D.gpu_flag && (y_sample = cu(y_sample))\n\n@time y = DFNO_3D.forward(model, θ, x_sample)\n@time y = DFNO_3D.forward(model, θ, x_sample)\n@time y = DFNO_3D.forward(model, θ, x_sample)\n\nMPI.Barrier()\n\nfunction loss_helper(params)\n    global loss = UTILS.dist_loss(DFNO_3D.forward(model, params, x_sample), y_sample)\n    return loss\nend\n\nrank == 0 && println(\"STARTED GRADIENT SCALING\")\n\n@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]\n@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]\n@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]\n\nMPI.Finalize()","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"If you have mpiexecjl set up, you can run the above by doing:","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"mpiexecjl --project=/path/to/your/environment -n NTASKS julia code_above.jl","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"OR if you have a HPC cluster with slurm set up, you can do:","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"salloc --gpus=NTASKS --time=01:00:00 --ntasks=NTASKS --gpus-per-task=1 --gpu-bind=none\nsrun julia --project=/path/to/your/environment code_above.jl","category":"page"},{"location":"examples/simple_3D/","page":"3D Forward and Gradient","title":"3D Forward and Gradient","text":"warning: Allocation\nYour salloc might look different based on your HPC cluster","category":"page"},{"location":"examples/training_2D/#Training-2D-Time-varying-FNO","page":"2D Training","title":"Training 2D Time varying FNO","text":"","category":"section"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"note: Jump right in\nTo get started, you can run some examples","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"Make sure to add necessary dependencies. You might also need to load a proper MPI implementation based on your hardware.","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"julia> ]\n(v1.9) activate /path/to/your/environment \n(venv) add JLD2 FileIO MAT MPI CUDA ParametricDFNOs","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"warning: To run on multiple GPUs\nIf you wish to run on multiple GPUs, make sure the GPUs are binded to different tasks. The approach we use is to unbind our GPUs on request and assign manually:CUDA.device!(rank % 4)which might be different if you have more or less than 4 GPUs per node. Also, make sure your MPI distribution is functional.","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"using MPI\nusing CUDA\nusing ParametricDFNOs.DFNO_2D\nusing ParametricDFNOs.UTILS\nusing JLD2, FileIO, MAT\n\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\npe_count = MPI.Comm_size(comm)\n\nglobal gpu_flag = parse(Bool, get(ENV, \"DFNO_2D_GPU\", \"0\"))\nDFNO_2D.set_gpu_flag(gpu_flag)\n\n# Julia requires you to manually assign the gpus, modify to your case.\nDFNO_2D.gpu_flag && (CUDA.device!(rank % 4))\npartition = [1, pe_count]\n\nmodelConfig = DFNO_2D.ModelConfig(nblocks=4, partition=partition)\n\n### Setup example dataset ###\n\nperm_path_mat = \"data/DFNO_2D/perm_gridspacing15.0.mat\"\nconc_path_mat = \"data/DFNO_2D/conc_gridspacing15.0.mat\"\nperm_store_path_jld2 = \"data/DFNO_2D/perm_gridspacing15.0.jld2\"\nconc_store_path_jld2 = \"data/DFNO_2D/conc_gridspacing15.0.jld2\"\n\n# TODO: Host a .jld2 file with correct dimensions\n# Check if .jld2 files already exist and skip processing if they do\nif isfile(perm_store_path_jld2) && isfile(conc_store_path_jld2)\n    rank == 0 && println(\"JLD2 files already exist, skipping processing.\")\nelseif rank == 0\n    ensure_directory = path -> isdir(path) || mkpath(path)\n    ensure_downloaded = (url, path) -> isfile(path) || run(`wget $url -q -O $path`)\n\n    # Ensure necessary directories exist\n    ensure_directory(dirname(perm_path_mat))\n    ensure_directory(dirname(perm_store_path_jld2))\n    \n    # Ensure .mat files are downloaded\n    ensure_downloaded(\"https://www.dropbox.com/s/o35wvnlnkca9r8k/perm_gridspacing15.0.mat\", perm_path_mat)\n    ensure_downloaded(\"https://www.dropbox.com/s/mzi0xgr0z3l553a/conc_gridspacing15.0.mat\", conc_path_mat)\n\n    # Load .mat files\n    perm = matread(perm_path_mat)[\"perm\"];\n    conc = matread(conc_path_mat)[\"conc\"];\n\n    conc = permutedims(conc, [2, 3, 1, 4])\n\n    # Save data to .jld2 format\n    @save perm_store_path_jld2 perm\n    @save conc_store_path_jld2 conc\nend\n\nMPI.Barrier(comm)\n\n#############################\n\ndataConfig = DFNO_2D.DataConfig(modelConfig=modelConfig, \n                                x_key = \"perm\",\n                                x_file = perm_store_path_jld2,\n                                y_key=\"conc\",\n                                y_file=conc_store_path_jld2)\n\nx_train, y_train, x_valid, y_valid = DFNO_2D.loadDistData(dataConfig)\n\ntrainConfig = DFNO_2D.TrainConfig(\n    epochs=10,\n    x_train=x_train,\n    y_train=y_train,\n    x_valid=x_valid,\n    y_valid=y_valid,\n    plot_every=1\n)\n\nmodel = DFNO_2D.Model(modelConfig)\nθ = DFNO_2D.initModel(model)\n\nDFNO_2D.train!(trainConfig, model, θ)\n\nMPI.Finalize()","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"If you have mpiexecjl set up, you can run the above by doing:","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"mpiexecjl --project=/path/to/your/environment -n NTASKS julia code_above.jl","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"OR if you have a HPC cluster with slurm set up, you can do:","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"salloc --gpus=NTASKS --time=01:00:00 --ntasks=NTASKS --gpus-per-task=1 --gpu-bind=none\nsrun julia --project=/path/to/your/environment code_above.jl","category":"page"},{"location":"examples/training_2D/","page":"2D Training","title":"2D Training","text":"warning: Allocation\nYour salloc might look different based on your HPC cluster","category":"page"},{"location":"api/DFNO_3D/#3D-Time-varying-FNO","page":"3D Time varying","title":"3D Time varying FNO","text":"","category":"section"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"tip: 3D Time varying\nThe implementation allows for discretization along time dimension to be 1 (only 1 time step). But you can also treat time as any other dimension, so this could also be as a generic 4D FNO","category":"page"},{"location":"api/DFNO_3D/#3D-Model","page":"3D Time varying","title":"3D Model","text":"","category":"section"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"Modules = [ParametricDFNOs.DFNO_3D]\nOrder  = [:type, :function]\nPages = [\"model.jl\"]","category":"page"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.Model","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.Model","text":"Model(config::ModelConfig)\n\nA mutable structure representing the FNO, including configurations, weights, biases, and other parameters.\n\nConstructor\n\nThis constructor initializes the model's layers and distributes the weights and biases according to the provided ModelConfig.\n\n\n\n\n\n","category":"type"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.ModelConfig","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.ModelConfig","text":"ModelConfig(;nx::Int=64, ny::Int=64, nz::Int=64, nt::Int=51, nc_in::Int=5, nc_mid::Int=128, nc_lift::Int=20, nc_out::Int=1, mx::Int=8, my::Int=8, mz::Int=8, mt::Int=4, nblocks::Int=4, dtype::DataType=Float32, partition::Vector{Int}=[1, 8], relu01::Bool=true)\n\nA configuration struct that holds parameters for the Model setup. \n\nFields\n\nnx: The discretization along the x-dimension.\nny: The discretization along the y-dimension.\nnz: The discretization along the z-dimension.\nnt: The discretization along time.\nnc_in: The number of input channels.\nnc_mid: The number of intermediate channels.\nnc_lift: The number of lifted channels.\nnc_out: The number of output channels.\nmx: The size of the model in the x-dimension for Fourier transform.\nmy: The size of the model in the y-dimension for Fourier transform.\nmz: The size of the model in the z-dimension for Fourier transform.\nmt: The size of the model in time for Fourier transform.\nnblocks: The number of blocks in the model.\ndtype: The data type used for computations, default is Float32.\npartition: The partitioning configuration for distributed computing, default is [1, 8].\nrelu01: Boolean that determines whether the final relu layer is applied\n\n\n\n\n\n","category":"type"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.initModel-Tuple{ParametricDFNOs.DFNO_3D.Model}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.initModel","text":"initModel(model::Model)\n\nInitializes the Model by allocating and setting the initial values for the parameters. \n\nArguments\n\nmodel: An instance of Model to be initialized.\n\nReturns\n\nReturns the initialized parameters θ which may be placed on GPU if the global gpu_flag is set to true.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_3D/#3D-Forward-Pass","page":"3D Time varying","title":"3D Forward Pass","text":"","category":"section"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"Modules = [ParametricDFNOs.DFNO_3D]\nOrder  = [:type, :function]\nPages = [\"forward.jl\"]","category":"page"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.forward-Tuple{ParametricDFNOs.DFNO_3D.Model, Any, Any}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.forward","text":"forward(model::Model, θ, x::Any)\n\nPerforms the forward pass using the model defined by Model.\n\nThe function applies a series of transformations to the input data x using the model parameters θ and the configurations within the model.\n\nArguments\n\nmodel: The Model object that contains configurations and parameters for the forward pass.\nθ: The parameters of the model, likely initialized by initModel.\nx: Input data that will be passed through the model.\n\nReturns\n\nThe output of the model after the forward pass, reshaped to the dimensions appropriate for the number of output channels, time steps, and spatial dimensions.\n\nDetails\n\nThe process includes:\n\nReshaping the input and applying lifting operations.\nProcessing through a series of blocks that includes spectral and standard convolutions, followed by batch normalization and non-linear activation functions (ReLU).\nFinal projection to the output channels and application of a double ReLU operation to finalize the forward pass.\n\nNotes\n\nThis function will move x to GPU and perform GPU-enabled computations if gpu_flag is set.\n\nCaution\n\nThe input x should have the correct number of elements but does not need to have any particular shape. However, incorrect permutation of the input dimensions will lead to incorrect solution operators.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_3D/#3D-Training","page":"3D Time varying","title":"3D Training","text":"","category":"section"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"Modules = [ParametricDFNOs.DFNO_3D]\nOrder  = [:type, :function]\nPages = [\"train.jl\"]","category":"page"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.TrainConfig","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.TrainConfig","text":"TrainConfig\n\nA configuration struct for setting up the training environment.\n\nFields\n\nnbatch: The number of samples per batch.\nepochs: The number of full training cycles through the dataset.\nseed: The random seed for reproducibility of shuffling and initialization.\nplot_every: Frequency of plotting the evaluation metrics (every n epochs).\nlearning_rate: The step size for the optimizer.\nx_train: Training input data.\ny_train: Training target data.\nx_valid: Validation input data.\ny_valid: Validation target data.\n\nUsage\n\nThis struct is used to encapsulate all necessary training parameters, including data and hyperparameters, to configure the training loop.\n\n\n\n\n\n","category":"type"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.train!-Tuple{ParametricDFNOs.DFNO_3D.TrainConfig, ParametricDFNOs.DFNO_3D.Model, Dict}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.train!","text":"train!(config::TrainConfig, model::Model, θ::Dict; comm=MPI.COMM_WORLD, plotEval::Function=plotEvaluation)\n\nConducts the training process for a given model using distributed computing and tracks the training and validation loss.\n\nArguments\n\nconfig: The training configuration settings as specified by the TrainConfig struct.\nmodel: The neural network model to be trained as specified by the Model struct.\nθ: A dictionary of model parameters to be optimized during training.\ncomm: The MPI communicator to be used for distributed training (defaults to MPI.COMM_WORLD).\nplotEval: The plotting function called to evaluate training progress (defaults to plotEvaluation).\n\nTraining Procedure\n\nThe function sets up the optimizer and initializes training and validation data.\nIt goes through the specified number of training epochs, updating model parameters via gradient descent.\nIf the gpu_flag is set, computations are performed on a GPU.\nAt specified intervals, the training process is evaluated by plotting the training and validation losses and predicted vs. actual outputs.\n\nOutputs\n\nUpdates the model's parameters in-place.\nProduces plots every epoch to visually evaluate model performance.\nSaves weights every 2 epochs\n\nNotes\n\nA progress meter will be displayed.\nMake sure to set up TrainConfig properly\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_3D/#3D-Data-Loading","page":"3D Time varying","title":"3D Data Loading","text":"","category":"section"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"warning: Critical component\nSee Data Partitioning for instructions on how to set it up properly.","category":"page"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"Modules = [ParametricDFNOs.DFNO_3D]\nOrder  = [:type, :function]\nPages = [\"data.jl\"]","category":"page"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.DataConfig","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.DataConfig","text":"DataConfig\n\nA struct for configuring the data loading process for model training and validation.\n\nFields\n\nntrain: Number of training samples.\nnvalid: Number of validation samples.\nx_key: Key under which input (X) data is stored in the JLD2 file.\nx_file: Path to the file containing input (X) data.\ny_key: Key under which output (Y) data is stored in the JLD2 file.\ny_file: Path to the file containing output (Y) data.\nmodelConfig: An instance of ModelConfig that contains model-specific configurations.\n\nDescription\n\nThis struct stores paths and keys for data files, along with the counts of training and validation samples, to facilitate data preparation and loading in a distributed computing environment. It is tightly coupled with the model's configuration, especially for partitioning the data across different processing nodes.\n\n\n\n\n\n","category":"type"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.loadDistData-Tuple{ParametricDFNOs.DFNO_3D.DataConfig}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.loadDistData","text":"loadDistData(config::DataConfig; dist_read_x_tensor=UTILS.dist_read_tensor, dist_read_y_tensor=UTILS.dist_read_tensor, comm=MPI.COMM_WORLD)\n\nLoads and distributes training and validation data across processes for distributed training.\n\nArguments\n\nconfig: An instance of DataConfig which holds configuration for data loading.\ndist_read_x_tensor: Function to read the distributed x tensors (defaults to dist_read_tensor).\ndist_read_y_tensor: Function to read the distributed y tensors (defaults to dist_read_tensor).\ncomm: MPI communicator used for distributed data loading (defaults to MPI.COMM_WORLD).\n\nFunctionality\n\nInitializes MPI communication to distribute data according to the model's partitioning scheme.\nLoads input and output data from specified files and keys, and distributes them according to the data partitioning logic defined in the model configuration.\nPrepares and separates the data into training and validation sets.\n\nReturns\n\nFour arrays: Training inputs, training outputs, validation inputs, and validation outputs, each formatted for the distributed training process.\nx_train, y_train, x_valid, y_valid\n\nThis function manages the distribution and partitioning of large datasets across multiple nodes in a parallel computing environment, using MPI for communication. It is essential for ensuring that data is appropriately sliced and distributed to match the computational architecture and memory constraints.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"tip: Distributed read for complex storage scenarios\nView Custom 3D Time varying FNO for an example of how you can extend this distributed read to a complex storage scheme.","category":"page"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"Modules = [ParametricDFNOs.DFNO_3D.UTILS]\nOrder  = [:type, :function]\nPages = [\"utils.jl\"]","category":"page"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.UTILS.dist_loss-Tuple{Any, Any}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.UTILS.dist_loss","text":"dist_loss(local_pred_y, local_true_y)\n\nCalculates the distributed normalized root mean squared error (NRMSE) between predicted and actual values.\n\nArguments\n\nlocal_pred_y: Local tensor of predicted values, typically a subset of the whole prediction corresponding to the data handled by a specific node.\nlocal_true_y: Local tensor of actual values corresponding to a subset of the data\n\nReturns\n\nA scalar value representing the relative L2 error of the predictions to the true values, which quantifies the prediction error normalized by the magnitude of the actual values.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.UTILS.dist_read_tensor-Tuple{Any, Any, Any}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.UTILS.dist_read_tensor","text":"dist_read_tensor(file_name, key, indices)\n\nReads a tensor slice from an HDF5 file based on provided indices and reshapes the result.\n\nArguments\n\nfile_name: The name or path of the HDF5 file from which data is to be read.\nkey: The key within the HDF5 file that corresponds to the dataset of interest.\nindices: The indices specifying the slice of the dataset to be extracted.\n\nReturns\n\nA reshaped array where the first dimension is singleton, extending the dimensions of the original data slice by one.\n\nFunctionality\n\nOpens an HDF5 file in read-only mode.\nAccesses the dataset associated with the provided key.\nExtracts the slice of the dataset specified by indices.\nReshapes the extracted data, adding a singleton dimension at the beginning.\n\nExample Usage\n\n# To read a specific slice from a dataset within an HDF5 file\ntensor_data = dist_read_tensor(\"data.h5\", \"dataset_key\", (1:10, 5:15, 2:2))\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_3D/#3D-Plotting","page":"3D Time varying","title":"3D Plotting","text":"","category":"section"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"Modules = [ParametricDFNOs.DFNO_3D]\nOrder  = [:type, :function]\nPages = [\"plot.jl\"]","category":"page"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.plotEvaluation-Tuple{ParametricDFNOs.DFNO_3D.ModelConfig, Any, Any, Any}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.plotEvaluation","text":"plotEvaluation(modelConfig::ModelConfig, x_plot, y_plot, y_predict; trainConfig::TrainConfig, additional::Dict{String,Any} = Dict{String,Any}())\n\nGenerates a plots comparing the true and predicted values along with the input data and the absolute difference magnified by a factor of 5 along the time dimension.\n\nArguments\n\nmodelConfig: A ModelConfig struct specifying the dimensions and parameters of the model.\nx_plot: Input data to the model.\ny_plot: True output data from the model.\ny_predict: Predicted output data from the model.\ntrainConfig: An optional TrainConfig struct containing training configurations, used for constructing the filename for the plot.\nadditional: An optional dictionary of additional objects that are added to the save file name.\n\nThis is a specific plotting function used for a 2 phase fluid flow problem, you can override this by passing your own plotting code that follows this method signature to train!\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_3D/#3D-Checkpoints","page":"3D Time varying","title":"3D Checkpoints","text":"","category":"section"},{"location":"api/DFNO_3D/","page":"3D Time varying","title":"3D Time varying","text":"Modules = [ParametricDFNOs.DFNO_3D]\nOrder  = [:type, :function]\nPages = [\"weights.jl\"]","category":"page"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.loadWeights!-NTuple{4, Any}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.loadWeights!","text":"loadWeights!(θ, filename, key, partition; comm=MPI.COMM_WORLD, isLocal=true)\n\nLoads and distributes weights across processes for a parallelized model.\n\nArguments\n\nθ: Dictionary of model parameters to be updated with loaded weights.\nfilename: Name or path of the file containing the saved weights.\nkey: Key under which the weights are saved in the file.\npartition: The partitioning scheme used for distributed tensor weights.\ncomm: MPI communicator for the distributed system (defaults to MPI.COMM_WORLD).\nisLocal: Flag indicating whether the file path should be under the generated 'weights' folder. (relative filepath or not)\n\nFunctionality\n\nLoads weights from a JLD2 file and distributes them according to the partitioning across MPI ranks.\nIf gpu_flag is set, ensures weights are moved to GPU memory.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_3D/#ParametricDFNOs.DFNO_3D.saveWeights-Tuple{Any, ParametricDFNOs.DFNO_3D.Model}","page":"3D Time varying","title":"ParametricDFNOs.DFNO_3D.saveWeights","text":"saveWeights(θ, model::Model; additional=Dict{String,Any}(), comm=MPI.COMM_WORLD)\n\nSaves the current state of the model's weights to a file, only executed by the rank 0 process.\n\nArguments\n\nθ: The current state of the model's parameters.\nmodel: The Model instance containing the model configurations.\nadditional: Include a Dict of strings that you would like your filename to contain and objects your file should contain\ncomm: The MPI communicator to be used for determining the process rank, can usually be ignored.\n\nFunctionality\n\nCollects distributed weights from all processes.\nSaves the weights to a JLD2 file with additional metadata.\n\nNotes\n\nThe file is saved with a unique name generated from model parameters and additional metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/#2D-Time-varying-FNO","page":"2D Time varying","title":"2D Time varying FNO","text":"","category":"section"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"tip: 2D Time varying\nThe implementation allows for discretization along time dimension to be 1 (only 1 time step). But you can also treat time as any other dimension, so this could also be as a generic 3D FNO","category":"page"},{"location":"api/DFNO_2D/#2D-Model","page":"2D Time varying","title":"2D Model","text":"","category":"section"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"Modules = [ParametricDFNOs.DFNO_2D]\nOrder  = [:type, :function]\nPages = [\"model.jl\"]","category":"page"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.Model","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.Model","text":"Model(config::ModelConfig)\n\nA mutable structure representing the FNO, including configurations, weights, biases, and other parameters.\n\nConstructor\n\nThis constructor initializes the model's layers and distributes the weights and biases according to the provided ModelConfig.\n\n\n\n\n\n","category":"type"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.ModelConfig","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.ModelConfig","text":"ModelConfig(;nx::Int=64, ny::Int=64, nt::Int=51, nc_in::Int=4, nc_mid::Int=128, nc_lift::Int=20, nc_out::Int=1, mx::Int=8, my::Int=8, mt::Int=4, nblocks::Int=4, dtype::DataType=Float32, partition::Vector{Int}=[1, 4])\n\nA configuration struct that holds parameters for the Model setup. \n\nFields\n\nnx: The discretization along the x-dimension.\nny: The discretization along the y-dimension.\nnt: The discretization along time.\nnc_in: The number of input channels.\nnc_mid: The number of intermediate channels.\nnc_lift: The number of lifted channels.\nnc_out: The number of output channels.\nmx: The size of the model in the x-dimension for Fourier transform.\nmy: The size of the model in the y-dimension for Fourier transform.\nmt: The size of the model in time for Fourier transform.\nnblocks: The number of blocks in the model.\ndtype: The data type used for computations, default is Float32.\npartition: The partitioning configuration for distributed computing, default is [1, 4].\n\n\n\n\n\n","category":"type"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.initModel-Tuple{ParametricDFNOs.DFNO_2D.Model}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.initModel","text":"initModel(model::Model)\n\nInitializes the Model by allocating and setting the initial values for the parameters. \n\nArguments\n\nmodel: An instance of Model to be initialized.\n\nReturns\n\nReturns the initialized parameters θ which may be placed on GPU if the global gpu_flag is set to true.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/#2D-Forward-Pass","page":"2D Time varying","title":"2D Forward Pass","text":"","category":"section"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"Modules = [ParametricDFNOs.DFNO_2D]\nOrder  = [:type, :function]\nPages = [\"forward.jl\"]","category":"page"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.forward-Tuple{ParametricDFNOs.DFNO_2D.Model, Any, Any}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.forward","text":"forward(model::Model, θ, x::Any)\n\nPerforms the forward pass using the model defined by Model.\n\nThe function applies a series of transformations to the input data x using the model parameters θ and the configurations within the model.\n\nArguments\n\nmodel: The Model object that contains configurations and parameters for the forward pass.\nθ: The parameters of the model, likely initialized by initModel.\nx: Input data that will be passed through the model.\n\nReturns\n\nThe output of the model after the forward pass, reshaped to the dimensions appropriate for the number of output channels, time steps, and spatial dimensions.\n\nDetails\n\nThe process includes:\n\nReshaping the input and applying lifting operations.\nProcessing through a series of blocks that includes spectral and standard convolutions, followed by batch normalization and non-linear activation functions (ReLU).\nFinal projection to the output channels and application of a double ReLU operation to finalize the forward pass.\n\nNotes\n\nThis function will move x to GPU and perform GPU-enabled computations if gpu_flag is set.\n\nCaution\n\nThe input x should have the correct number of elements but does not need to have any particular shape. However, incorrect permutation of the input dimensions will lead to incorrect solution operators.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/#2D-Training","page":"2D Time varying","title":"2D Training","text":"","category":"section"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"Modules = [ParametricDFNOs.DFNO_2D]\nOrder  = [:type, :function]\nPages = [\"train.jl\"]","category":"page"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.TrainConfig","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.TrainConfig","text":"TrainConfig\n\nA configuration struct for setting up the training environment.\n\nFields\n\nnbatch: The number of samples per batch.\nepochs: The number of full training cycles through the dataset.\nseed: The random seed for reproducibility of shuffling and initialization.\nplot_every: Frequency of plotting the evaluation metrics (every n epochs).\nlearning_rate: The step size for the optimizer.\nx_train: Training input data.\ny_train: Training target data.\nx_valid: Validation input data.\ny_valid: Validation target data.\n\nUsage\n\nThis struct is used to encapsulate all necessary training parameters, including data and hyperparameters, to configure the training loop.\n\n\n\n\n\n","category":"type"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.train!-Tuple{ParametricDFNOs.DFNO_2D.TrainConfig, ParametricDFNOs.DFNO_2D.Model, Dict}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.train!","text":"train!(config::TrainConfig, model::Model, θ::Dict; comm=MPI.COMM_WORLD, plotEval::Function=plotEvaluation)\n\nConducts the training process for a given model using distributed computing and tracks the training and validation loss.\n\nArguments\n\nconfig: The training configuration settings as specified by the TrainConfig struct.\nmodel: The neural network model to be trained as specified by the Model struct.\nθ: A dictionary of model parameters to be optimized during training.\ncomm: The MPI communicator to be used for distributed training (defaults to MPI.COMM_WORLD).\nplotEval: The plotting function called to evaluate training progress (defaults to plotEvaluation).\n\nTraining Procedure\n\nThe function sets up the optimizer and initializes training and validation data.\nIt goes through the specified number of training epochs, updating model parameters via gradient descent.\nIf the gpu_flag is set, computations are performed on a GPU.\nAt specified intervals, the training process is evaluated by plotting the training and validation losses and predicted vs. actual outputs.\n\nOutputs\n\nUpdates the model's parameters in-place.\nProduces plots every epoch to visually evaluate model performance.\nSaves weights every 2 epochs\n\nNotes\n\nA progress meter will be displayed.\nMake sure to set up TrainConfig properly\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/#2D-Data-Loading","page":"2D Time varying","title":"2D Data Loading","text":"","category":"section"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"warning: Critical component\nSee Data Partitioning for instructions on how to set it up properly.","category":"page"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"Modules = [ParametricDFNOs.DFNO_2D]\nOrder  = [:type, :function]\nPages = [\"data.jl\"]","category":"page"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.DataConfig","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.DataConfig","text":"DataConfig\n\nA struct for configuring the data loading process for model training and validation.\n\nFields\n\nntrain: Number of training samples.\nnvalid: Number of validation samples.\nx_key: Key under which input (X) data is stored in the JLD2 file.\nx_file: Path to the file containing input (X) data.\ny_key: Key under which output (Y) data is stored in the JLD2 file.\ny_file: Path to the file containing output (Y) data.\nmodelConfig: An instance of ModelConfig that contains model-specific configurations.\n\nDescription\n\nThis struct stores paths and keys for data files, along with the counts of training and validation samples, to facilitate data preparation and loading in a distributed computing environment. It is tightly coupled with the model's configuration, especially for partitioning the data across different processing nodes.\n\n\n\n\n\n","category":"type"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.loadDistData-Tuple{ParametricDFNOs.DFNO_2D.DataConfig}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.loadDistData","text":"loadDistData(config::DataConfig; dist_read_x_tensor=UTILS.dist_read_tensor, dist_read_y_tensor=UTILS.dist_read_tensor, comm=MPI.COMM_WORLD)\n\nLoads and distributes training and validation data across processes for distributed training.\n\nArguments\n\nconfig: An instance of DataConfig which holds configuration for data loading.\ndist_read_x_tensor: Function to read the distributed x tensors (defaults to dist_read_tensor).\ndist_read_y_tensor: Function to read the distributed y tensors (defaults to dist_read_tensor).\ncomm: MPI communicator used for distributed data loading (defaults to MPI.COMM_WORLD).\n\nFunctionality\n\nInitializes MPI communication to distribute data according to the model's partitioning scheme.\nLoads input and output data from specified files and keys, and distributes them according to the data partitioning logic defined in the model configuration.\nPrepares and separates the data into training and validation sets.\n\nReturns\n\nFour arrays: Training inputs, training outputs, validation inputs, and validation outputs, each formatted for the distributed training process.\nx_train, y_train, x_valid, y_valid\n\nThis function manages the distribution and partitioning of large datasets across multiple nodes in a parallel computing environment, using MPI for communication. It is essential for ensuring that data is appropriately sliced and distributed to match the computational architecture and memory constraints.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"Modules = [ParametricDFNOs.DFNO_2D.UTILS]\nOrder  = [:type, :function]\nPages = [\"utils.jl\"]","category":"page"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.UTILS.dist_loss-Tuple{Any, Any}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.UTILS.dist_loss","text":"dist_loss(local_pred_y, local_true_y)\n\nCalculates the distributed normalized root mean squared error (NRMSE) between predicted and actual values.\n\nArguments\n\nlocal_pred_y: Local tensor of predicted values, typically a subset of the whole prediction corresponding to the data handled by a specific node.\nlocal_true_y: Local tensor of actual values corresponding to a subset of the data\n\nReturns\n\nA scalar value representing the relative L2 error of the predictions to the true values, which quantifies the prediction error normalized by the magnitude of the actual values.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.UTILS.dist_read_tensor-Tuple{Any, Any, Any}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.UTILS.dist_read_tensor","text":"dist_read_tensor(file_name, key, indices)\n\nReads a tensor slice from an HDF5 file based on provided indices and reshapes the result.\n\nArguments\n\nfile_name: The name or path of the HDF5 file from which data is to be read.\nkey: The key within the HDF5 file that corresponds to the dataset of interest.\nindices: The indices specifying the slice of the dataset to be extracted.\n\nReturns\n\nA reshaped array where the first dimension is singleton, extending the dimensions of the original data slice by one.\n\nFunctionality\n\nOpens an HDF5 file in read-only mode.\nAccesses the dataset associated with the provided key.\nExtracts the slice of the dataset specified by indices.\nReshapes the extracted data, adding a singleton dimension at the beginning.\n\nExample Usage\n\n# To read a specific slice from a dataset within an HDF5 file\ntensor_data = dist_read_tensor(\"data.h5\", \"dataset_key\", (1:10, 5:15, 2:2))\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/#2D-Plotting","page":"2D Time varying","title":"2D Plotting","text":"","category":"section"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"Modules = [ParametricDFNOs.DFNO_2D]\nOrder  = [:type, :function]\nPages = [\"plot.jl\"]","category":"page"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.plotEvaluation-Tuple{ParametricDFNOs.DFNO_2D.ModelConfig, Any, Any, Any}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.plotEvaluation","text":"plotEvaluation(modelConfig::ModelConfig, x_plot, y_plot, y_predict; trainConfig::TrainConfig, additional::Dict{String,Any} = Dict{String,Any}())\n\nGenerates a plots comparing the true and predicted values along with the input data and the absolute difference magnified by a factor of 5 along the time dimension.\n\nArguments\n\nmodelConfig: A ModelConfig struct specifying the dimensions and parameters of the model.\nx_plot: Input data to the model.\ny_plot: True output data from the model.\ny_predict: Predicted output data from the model.\ntrainConfig: An optional TrainConfig struct containing training configurations, used for constructing the filename for the plot.\nadditional: An optional dictionary of additional objects that are added to the save file name.\n\nThis is a specific plotting function used for a 2 phase fluid flow problem, you can override this by passing your own plotting code that follows this method signature to train!\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/#2D-Checkpoints","page":"2D Time varying","title":"2D Checkpoints","text":"","category":"section"},{"location":"api/DFNO_2D/","page":"2D Time varying","title":"2D Time varying","text":"Modules = [ParametricDFNOs.DFNO_2D]\nOrder  = [:type, :function]\nPages = [\"weights.jl\"]","category":"page"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.loadWeights!-NTuple{4, Any}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.loadWeights!","text":"loadWeights!(θ, filename, key, partition; comm=MPI.COMM_WORLD, isLocal=true)\n\nLoads and distributes weights across processes for a parallelized model.\n\nArguments\n\nθ: Dictionary of model parameters to be updated with loaded weights.\nfilename: Name or path of the file containing the saved weights.\nkey: Key under which the weights are saved in the file.\npartition: The partitioning scheme used for distributed tensor weights.\ncomm: MPI communicator for the distributed system (defaults to MPI.COMM_WORLD).\nisLocal: Flag indicating whether the file path should be under the generated 'weights' folder. (relative filepath or not)\n\nFunctionality\n\nLoads weights from a JLD2 file and distributes them according to the partitioning across MPI ranks.\nIf gpu_flag is set, ensures weights are moved to GPU memory.\n\n\n\n\n\n","category":"method"},{"location":"api/DFNO_2D/#ParametricDFNOs.DFNO_2D.saveWeights-Tuple{Any, ParametricDFNOs.DFNO_2D.Model}","page":"2D Time varying","title":"ParametricDFNOs.DFNO_2D.saveWeights","text":"saveWeights(θ, model::Model; additional=Dict{String,Any}(), comm=MPI.COMM_WORLD)\n\nSaves the current state of the model's weights to a file, only executed by the rank 0 process.\n\nArguments\n\nθ: The current state of the model's parameters.\nmodel: The Model instance containing the model configurations.\nadditional: Include a Dict of strings that you would like your filename to contain and objects your file should contain\ncomm: The MPI communicator to be used for determining the process rank, can usually be ignored.\n\nFunctionality\n\nCollects distributed weights from all processes.\nSaves the weights to a JLD2 file with additional metadata.\n\nNotes\n\nThe file is saved with a unique name generated from model parameters and additional metadata.\n\n\n\n\n\n","category":"method"},{"location":"api/UTILS/#Utilities","page":"Utilites","title":"Utilities","text":"","category":"section"},{"location":"api/UTILS/","page":"Utilites","title":"Utilites","text":"note: Distributed Loss Function\nWe provide a distributed relative L2 loss but most distributed loss functions should be straight-forward to build with ParametricOperators.jl","category":"page"},{"location":"api/UTILS/","page":"Utilites","title":"Utilites","text":"Modules = [ParametricDFNOs.UTILS]\nOrder  = [:type, :function]\nPages = [\"utils.jl\"]","category":"page"},{"location":"api/UTILS/#ParametricDFNOs.UTILS.dist_loss-Tuple{Any, Any}","page":"Utilites","title":"ParametricDFNOs.UTILS.dist_loss","text":"dist_loss(local_pred_y, local_true_y)\n\nCalculates the distributed normalized root mean squared error (NRMSE) between predicted and actual values.\n\nArguments\n\nlocal_pred_y: Local tensor of predicted values, typically a subset of the whole prediction corresponding to the data handled by a specific node.\nlocal_true_y: Local tensor of actual values corresponding to a subset of the data\n\nReturns\n\nA scalar value representing the relative L2 error of the predictions to the true values, which quantifies the prediction error normalized by the magnitude of the actual values.\n\n\n\n\n\n","category":"method"},{"location":"api/UTILS/#ParametricDFNOs.UTILS.dist_read_tensor-Tuple{Any, Any, Any}","page":"Utilites","title":"ParametricDFNOs.UTILS.dist_read_tensor","text":"dist_read_tensor(file_name, key, indices)\n\nReads a tensor slice from an HDF5 file based on provided indices and reshapes the result.\n\nArguments\n\nfile_name: The name or path of the HDF5 file from which data is to be read.\nkey: The key within the HDF5 file that corresponds to the dataset of interest.\nindices: The indices specifying the slice of the dataset to be extracted.\n\nReturns\n\nA reshaped array where the first dimension is singleton, extending the dimensions of the original data slice by one.\n\nFunctionality\n\nOpens an HDF5 file in read-only mode.\nAccesses the dataset associated with the provided key.\nExtracts the slice of the dataset specified by indices.\nReshapes the extracted data, adding a singleton dimension at the beginning.\n\nExample Usage\n\n# To read a specific slice from a dataset within an HDF5 file\ntensor_data = dist_read_tensor(\"data.h5\", \"dataset_key\", (1:10, 5:15, 2:2))\n\n\n\n\n\n","category":"method"},{"location":"api/UTILS/#GPU-Helpers","page":"Utilites","title":"GPU Helpers","text":"","category":"section"},{"location":"api/UTILS/","page":"Utilites","title":"Utilites","text":"Modules = [ParametricDFNOs.DFNO_2D]\nOrder  = [:type, :function]\nPages = [\"DFNO_2D.jl\"]","category":"page"},{"location":"api/UTILS/#ParametricDFNOs.DFNO_2D.set_gpu_flag-Tuple{Bool}","page":"Utilites","title":"ParametricDFNOs.DFNO_2D.set_gpu_flag","text":"set_gpu_flag(flag::Bool)\n\nFunction to set the gpu_flag and update device accordingly. Should be set at the beginning of your script. All FNO computatation following this will use the device set.\n\n\n\n\n\n","category":"method"},{"location":"api/UTILS/","page":"Utilites","title":"Utilites","text":"Modules = [ParametricDFNOs.DFNO_3D]\nOrder  = [:type, :function]\nPages = [\"DFNO_3D.jl\"]","category":"page"},{"location":"api/UTILS/#ParametricDFNOs.DFNO_3D.set_gpu_flag-Tuple{Bool}","page":"Utilites","title":"ParametricDFNOs.DFNO_3D.set_gpu_flag","text":"set_gpu_flag(flag::Bool)\n\nFunction to set the gpu_flag and update device accordingly. Should be set at the beginning of your script. All FNO computatation following this will use the device set.\n\n\n\n\n\n","category":"method"},{"location":"future/#Future-Work","page":"Future Work","title":"Future Work","text":"","category":"section"},{"location":"future/","page":"Future Work","title":"Future Work","text":"Add distributed support for multiple variations of the FNO such as","category":"page"},{"location":"future/","page":"Future Work","title":"Future Work","text":"Mixed Precision FNO (Colin White et al., 2023)\nGINO (Zongyi Li et al., 2023)\nMGTFNO (Jean Kossaifi et al., 2023)\nGANO (Md Ashiqur Rahman et al., 2022)\nAFNO (John Guibas et al., 2021)\nUFNO (Gege Wen et al., 2021)","category":"page"},{"location":"citation/#Citation","page":"Citation","title":"Citation","text":"","category":"section"},{"location":"citation/","page":"Citation","title":"Citation","text":"If you use ParametricDFNOs.jl, please cite the following:","category":"page"},{"location":"citation/","page":"Citation","title":"Citation","text":"@presentation {rex2023ML4SEISMIClsp,\n\ttitle = {Large-scale parametric PDE approximations with model-parallel Fourier neural operators},\n\tyear = {2023},\n\tmonth = {11},\n\turl = {https://slim.gatech.edu/Publications/Public/Conferences/ML4SEISMIC/2023/rex2023ML4SEISMIClsp},\n\tauthor = {Richard Rex and Thomas J. Grady II and Rishi Khan and Ziyi Yin and Felix J. Herrmann}\n}","category":"page"},{"location":"quickstart/#Installation","page":"Quick Start","title":"Installation","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Add ParametricDFNOs.jl as a dependency to your environment.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"To add, either do:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"julia> ]\n(v1.9) add ParametricDFNOs","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"OR","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"julia> using Pkg\njulia> Pkg.activate(\"path/to/your/environment\")\njulia> Pkg.add(\"ParametricDFNOs\")","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"note: Jump right in\nTo get started, you can also try running some examples","category":"page"},{"location":"quickstart/#Setup","page":"Quick Start","title":"Setup","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Make sure to include the right dependency you plan on using in your environment","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"using MPI\nusing CUDA\n\n# If you plan on using the 2D Time varying FNO or 3D FNO.\nusing ParametricDFNOs.DFNO_2D\n\n# If you plan on using the 3D Time varying FNO or 4D FNO.\nusing ParametricDFNOs.DFNO_3D","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"We also use PyPlot, so you would need to do:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"python3 -m pip install matplotlib","category":"page"},{"location":"quickstart/#MPI-setup","page":"Quick Start","title":"MPI setup","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"note: MPI Distribution\nMake sure you have a functional MPI Distribution set up","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"All code must be wrapped in:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"MPI.Init()\n\n### Code here ###\n\nMPI.Finalize()","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"tip: Change to custom use case\nWe show the usage for ParametricDFNOs.DFNO_2D but the extension to the other FNOs should be as simple as changing the number. Please refer to the API for exact differences.","category":"page"},{"location":"quickstart/#GPU-usage","page":"Quick Start","title":"GPU usage","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"note: Default behavior\nBy default, the package will be set to use the GPU based the whether the DFNO_2D_GPU flag was set during compile time of the package","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"You can set the GPU flag by using:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"export DFNO_2D_GPU=1","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"and ","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"global gpu_flag = parse(Bool, get(ENV, \"DFNO_2D_GPU\", \"0\"))\nDFNO_2D.set_gpu_flag(gpu_flag)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"warning: Binding GPUs\nIf you wish to run on multiple GPUs, make sure the GPUs are binded to different tasks. The approach we choose in our examples is to unbind our GPUs on request and assign manually:using CUDA\n\nCUDA.device!(rank % 4)which might be different if you have more or less than 4 GPUs per node.","category":"page"},{"location":"quickstart/#Model-Setup","page":"Quick Start","title":"Model Setup","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Define a 2D Model configuration:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"modelConfig = DFNO_2D.ModelConfig(nx=20, ny=20, nt=50, mx=4, my=4, mt=4, nblocks=4, partition=partition, dtype=Float32)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Define some random inputs to operate on:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"input_size = (modelConfig.nc_in * modelConfig.nx * modelConfig.ny * modelConfig.nt) ÷ prod(partition)\noutput_size = input_size * modelConfig.nc_out ÷ modelConfig.nc_in\n\nx = rand(modelConfig.dtype, input_size, 1)\ny = rand(modelConfig.dtype, output_size, 1)","category":"page"},{"location":"quickstart/#Initializing-model","page":"Quick Start","title":"Initializing model","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"model = DFNO_2D.Model(modelConfig)\nθ = DFNO_2D.initModel(model)","category":"page"},{"location":"quickstart/#Forward-and-backward-pass","page":"Quick Start","title":"Forward and backward pass","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"See Simple 2D forward and gradient pass for a full example.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"DFNO_2D.forward(model, θ, x)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"note: Distributed Loss Function\nWe provide a distributed relative L2 loss but most distributed loss functions should be straight-forward to build with ParametricOperators.jl","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"To compute gradient:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"using Zygote\nusing ParametricDFNOs.UTILS\n\ngradient(params -> loss_helper(UTILS.dist_loss(DFNO_2D.forward(model, params, x), y)), θ)[1]","category":"page"},{"location":"quickstart/#Data-Partitioning","page":"Quick Start","title":"Data Partitioning","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"We have a 2D Data Loading struct to store information about our data, consider Training 2D Time varying FNO","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"dataConfig = DFNO_2D.DataConfig(modelConfig=modelConfig, \n                                x_key = \"perm\",\n                                x_file = perm_store_path_jld2,\n                                y_key=\"conc\",\n                                y_file = conc_store_path_jld2)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Consider the following dimensions:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"c, t, x, y, z where","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"x, y, z - Spatial Dimensions\nt - Time Dimension\nc - Channel Dimension","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Data is considered to be combined along certain dimensions:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"ct, xy for DFNO_2D","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"ctx, yz for DFNO_3D","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"The partition array which is a two dimensional array specifies across how many workers is a given combined dimension split across.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"By default we do:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"comm = MPI.COMM_WORLD\npe_count = MPI.Comm_size(comm)\n\npartition = [1, pe_count]","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"The models are implemented to modify the operators according to the specified partition. We suggest you leave this as it is.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"warning: Running into assertion errors\nIf you run into any assertion errors that the number of workers do not divide the data evenly, please make a github issue","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"We provide a distributed read wrapper which allows you to read data seamlessly.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Simply implement:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"\n# Returns tensor of size (in_channels, size(indices)...)\nfunction dist_read_x_tensor(file_name, key, indices)\n\n# Returns tensor of size (out_channels=1, size(indices)...)\nfunction dist_read_y_tensor(file_name, key, indices)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"note: in channels\nThe number of in_channels you specify at Model Setup is data_channels + 3 for DFNO_2D and  data_channels + 4 for DFNO_3D. This is to account for the grid data we include for each of the dimensions in the FNO.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"warning: out channels\nCurrently, distributed wrapper only supports reading for the case where out channel is 1. You can implement your own read function or wait for a version update","category":"page"},{"location":"quickstart/#DFNO_2D","page":"Quick Start","title":"DFNO_2D","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Here, indicies for the dist_read_x_tensor represents ","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"(x_start:x_end, y_start:y_end, sample_start:sample_end)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"and the indices for the dist_read_y_tensor represents:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"(x_start:x_end, y_start:y_end, t_start:t_end, sample_start:sample_end)","category":"page"},{"location":"quickstart/#DFNO_3D","page":"Quick Start","title":"DFNO_3D","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Here, indicies for the dist_read_x_tensor represents ","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"(x_start:x_end, y_start:y_end, z_start:z_end, sample_start:sample_end)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"and the indices for the dist_read_y_tensor represents:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"(x_start:x_end, y_start:y_end, z_start:z_end, t_start:t_end, sample_start:sample_end)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Now you can use loadDistData from 2D Data Loading or 3D Data Loading","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"This can also be extended to complex storage regime. Consider the following case:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"samples/\n├── sample1/\n│   ├── inputs.jld2\n│   └── outputs.jld2\n└── sample2/\n    ├── inputs.jld2\n    └── outputs.jld2","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"We can do Custom 3D Time varying FNO","category":"page"},{"location":"quickstart/#Training-wrapper","page":"Quick Start","title":"Training wrapper","text":"","category":"section"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"We also provide a training wrapper to train out the box. See Training 2D Time varying FNO for a full example.","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"Define a 2D Training configuration:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"trainConfig = DFNO_2D.TrainConfig(\n    epochs=10,\n    x_train=x_train,\n    y_train=y_train,\n    x_valid=x_valid,\n    y_valid=y_valid,\n    plot_every=1\n)","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"And train using:","category":"page"},{"location":"quickstart/","page":"Quick Start","title":"Quick Start","text":"DFNO_2D.train!(trainConfig, model, θ)","category":"page"},{"location":"examples/custom_3D/#Custom-3D-Time-varying-FNO","page":"3D Custom Dataset","title":"Custom 3D Time varying FNO","text":"","category":"section"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"warning: Not a executable example\nWe provide this example to understand how to use the loadDistData function from 2D Data Loading and 3D Data Loading to perform distributed reading with any custom dataset.","category":"page"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"See Data Partitioning for how partitioning is handled.","category":"page"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"In this case, the data is stored in the following format:","category":"page"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"samples/\n├── sample1/\n│   ├── inputs.jld2\n│   └── outputs.jld2\n└── sample2/\n    ├── inputs.jld2\n    └── outputs.jld2","category":"page"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"We define a distributed read by providing dist_read_x_tensor and dist_read_y_tensor.","category":"page"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"data.jl:","category":"page"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"using HDF5\nusing ParametricDFNOs.DFNO_3D\n\n#### PERLMUTTER Data Loading ####\n\nfunction read_perlmutter_data(path::String, modelConfig::ModelConfig, rank::Int; ntrain::Int=1000, nvalid::Int=100)\n\n    n = ntrain + nvalid\n\n    function read_x_tensor(file_name, key, indices)\n        # indices for xyzn -> cxyzn where c=n=1 (t gets introduced and broadcasted later)\n        data = nothing\n        h5open(file_name, \"r\") do file\n            dataset = file[key]\n            data = dataset[indices[1:3]...]\n        end\n        return reshape(data, 1, (size(data)...), 1)\n    end\n    \n    function read_y_tensor(file_name, key, indices)\n        # indices for xyztn -> cxyztn where c=n=1\n        data = zeros(modelConfig.dtype, map(range -> length(range), indices[1:4]))\n        h5open(file_name, \"r\") do file\n            times = file[key]\n            for t in indices[4]\n                data[:, :, :, t - indices[4][1] + 1] = file[times[t]][indices[1:3]...]\n            end\n        end\n        return reshape(data, 1, (size(data)...), 1)\n    end\n    \n    x_train = zeros(modelConfig.dtype, modelConfig.nc_in * modelConfig.nt * modelConfig.nx ÷ modelConfig.partition[1], modelConfig.ny * modelConfig.nz ÷ modelConfig.partition[2], ntrain)\n    y_train = zeros(modelConfig.dtype, modelConfig.nc_out * modelConfig.nt * modelConfig.nx ÷ modelConfig.partition[1], modelConfig.ny * modelConfig.nz ÷ modelConfig.partition[2], ntrain)\n    x_valid = zeros(modelConfig.dtype, modelConfig.nc_in * modelConfig.nt * modelConfig.nx ÷ modelConfig.partition[1], modelConfig.ny * modelConfig.nz ÷ modelConfig.partition[2], nvalid)\n    y_valid = zeros(modelConfig.dtype, modelConfig.nc_out * modelConfig.nt * modelConfig.nx ÷ modelConfig.partition[1], modelConfig.ny * modelConfig.nz ÷ modelConfig.partition[2], nvalid)\n\n    idx = 1\n\n    for entry in readdir(path; join=true)\n        try\n            x_file = entry * \"/inputs.jld2\"\n            y_file = entry * \"/outputs.jld2\"\n\n            dataConfig = DFNO_3D.DataConfig(modelConfig=modelConfig, \n                                            ntrain=1, \n                                            nvalid=0, \n                                            x_file=x_file,\n                                            y_file=y_file,\n                                            x_key=\"K\",\n                                            y_key=\"saturations\")\n\n            x, y, _, _ = DFNO_3D.loadDistData(dataConfig, \n            dist_read_x_tensor=read_x_tensor, dist_read_y_tensor=read_y_tensor)\n\n            if idx <= ntrain\n                x_train[:,:,idx] = x[:,:,1]\n                y_train[:,:,idx] = y[:,:,1]\n            else\n                x_valid[:,:,idx-ntrain] = x[:,:,1]\n                y_valid[:,:,idx-ntrain] = y[:,:,1]\n            end\n            (rank == 0) && println(\"Loaded data sample no. $(idx) / $(n)\")\n            idx == n && break\n            idx += 1\n        catch e\n            (rank == 0) && println(\"Failed to load data sample no. $(idx). Error: $e\")\n            continue\n        end\n    end\n\n    return x_train, y_train, x_valid, y_valid\nend","category":"page"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"We can now use this in our normal training regime such as Training 2D Time varying FNO by doing:","category":"page"},{"location":"examples/custom_3D/","page":"3D Custom Dataset","title":"3D Custom Dataset","text":"using MPI\nusing CUDA\nusing ParametricDFNOs.DFNO_3D\nusing ParametricDFNOs.UTILS\n\ninclude(\"data.jl\")\n\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\npe_count = MPI.Comm_size(comm)\n\nglobal gpu_flag = parse(Bool, get(ENV, \"DFNO_3D_GPU\", \"0\"))\nDFNO_3D.set_gpu_flag(gpu_flag)\n\n# Julia requires you to manually assign the gpus, modify to your case.\nDFNO_3D.gpu_flag && (CUDA.device!(rank % 4))\npartition = [1, pe_count]\n\nnblocks, dim, md, mt, ntrain, nvalid, nbatch, epochs = parse.(Int, ARGS[1:8])\n\n@assert MPI.Comm_size(comm) == prod(partition)\n\nmodelConfig = DFNO_3D.ModelConfig(nx=dim, ny=dim, nz=dim, mx=md, my=md, mz=md, mt=mt, nblocks=nblocks, partition=partition, dtype=Float32)\n\ndataset_path = \"/pscratch/sd/r/richardr/v5/$(dim)³\"\n\nx_train, y_train, x_valid, y_valid = read_perlmutter_data(dataset_path, modelConfig, MPI.Comm_rank(comm), ntrain=ntrain, nvalid=nvalid)\n\nmodel = DFNO_3D.Model(modelConfig)\nθ = DFNO_3D.initModel(model)\n\ntrainConfig = DFNO_3D.TrainConfig(\n    epochs=epochs,\n    x_train=x_train,\n    y_train=y_train,\n    x_valid=x_valid,\n    y_valid=y_valid,\n    plot_every=1,\n    nbatch=nbatch\n)\n\nDFNO_3D.train!(trainConfig, model, θ)\n\nMPI.Finalize()","category":"page"},{"location":"examples/simple_2D/#Simple-2D-forward-and-gradient-pass","page":"2D Forward and Gradient","title":"Simple 2D forward and gradient pass","text":"","category":"section"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"note: Jump right in\nTo get started, you can run some examples","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"Make sure to add necessary dependencies. You might also need to load a proper MPI implementation based on your hardware.","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"julia> ]\n(v1.9) activate /path/to/your/environment \n(venv) add MPI Zygote CUDA ParametricDFNOs","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"warning: To run on multiple GPUs\nIf you wish to run on multiple GPUs, make sure the GPUs are binded to different tasks. The approach we use is to unbind our GPUs on request and assign manually:CUDA.device!(rank % 4)which might be different if you have more or less than 4 GPUs per node. Also, make sure your MPI distribution is functional.","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"using MPI\nusing CUDA\nusing Zygote\nusing ParametricDFNOs.DFNO_2D\nusing ParametricDFNOs.UTILS\n\nMPI.Init()\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\npe_count = MPI.Comm_size(comm)\n\nglobal gpu_flag = parse(Bool, get(ENV, \"DFNO_2D_GPU\", \"0\"))\nDFNO_2D.set_gpu_flag(gpu_flag)\n\n# Julia requires you to manually assign the gpus, modify to your case.\nDFNO_2D.gpu_flag && (CUDA.device!(rank % 4))\npartition = [1, pe_count]\n\nnx, ny, nt = 20, 20, 30\nmodes, nblocks = 8, 4\n\n@assert MPI.Comm_size(comm) == prod(partition)\nmodelConfig = DFNO_2D.ModelConfig(nx=nx, ny=ny, nt=nt, mx=modes, my=modes, mt=modes, nblocks=nblocks, partition=partition, dtype=Float32)\n\nmodel = DFNO_2D.Model(modelConfig)\nθ = DFNO_2D.initModel(model)\n\ninput_size = (model.config.nc_in * model.config.nx * model.config.ny * model.config.nt) ÷ prod(partition)\noutput_size = input_size * model.config.nc_out ÷ model.config.nc_in\n\nx_sample = rand(modelConfig.dtype, input_size, 1)\ny_sample = rand(modelConfig.dtype, output_size, 1)\n\nDFNO_2D.gpu_flag && (y_sample = cu(y_sample))\n\n@time y = DFNO_2D.forward(model, θ, x_sample)\n@time y = DFNO_2D.forward(model, θ, x_sample)\n@time y = DFNO_2D.forward(model, θ, x_sample)\n\nMPI.Barrier()\n\nfunction loss_helper(params)\n    global loss = UTILS.dist_loss(DFNO_2D.forward(model, params, x_sample), y_sample)\n    return loss\nend\n\nrank == 0 && println(\"STARTED GRADIENT SCALING\")\n\n@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]\n@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]\n@time grads_time = @elapsed gradient(params -> loss_helper(params), θ)[1]\n\nMPI.Finalize()","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"If you have mpiexecjl set up, you can run the above by doing:","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"mpiexecjl --project=/path/to/your/environment -n NTASKS julia code_above.jl","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"OR if you have a HPC cluster with slurm set up, you can do:","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"salloc --gpus=NTASKS --time=01:00:00 --ntasks=NTASKS --gpus-per-task=1 --gpu-bind=none\nsrun julia --project=/path/to/your/environment code_above.jl","category":"page"},{"location":"examples/simple_2D/","page":"2D Forward and Gradient","title":"2D Forward and Gradient","text":"warning: Allocation\nYour salloc might look different based on your HPC cluster","category":"page"},{"location":"#ParametricDFNOs.jl","page":"Introduction","title":"ParametricDFNOs.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"ParametricDFNOs.jl is an library designed for training large scale Fourier Neural Operators on multiple nodes. It is network that is primarily used to learn solution operators to PDE systems (Zongyi Li, et al., 2020). We adopt a model parallel architecture (Grady et al., 2022) to offer a clean and easy to use implementation of Distributed Fourier Neural Operators in Julia.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"note: Acknowledgement\nParametricDFNOs.jl is a library built on top of ParametricOperators.jl, a kronecker based framework that allows for efficient machine learning on large scale data","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Read our paper here.","category":"page"},{"location":"#Authors","page":"Introduction","title":"Authors","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This package is developed and maintained by Felix J. Herrmann's SlimGroup at Georgia Institute of Technology. The main contributors of this package are:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Richard Rex","category":"page"},{"location":"#License","page":"Introduction","title":"License","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"MIT License\n\nCopyright (c) 2024 SLIM Group @ Georgia Institute of Technology\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.","category":"page"}]
}
